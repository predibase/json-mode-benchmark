{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982ba058-ec68-4159-835b-81e54dfccd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, Counter\n",
    "from lorax import Client\n",
    "from pydantic import BaseModel\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "import json\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a38e68-4561-49e4-bedd-b8053b4122a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Your task is a Named Entity Recognition (NER) task. Predict the category of\n",
    "each entity, then place the entity into the list associated with the \n",
    "category in an output JSON payload. Below is an example:\n",
    "\n",
    "Input: EU rejects German call to boycott British lamb . Output: {{\"person\":\n",
    "[], \"organization\": [\"EU\"], \"location\": [], \"miscellaneous\": [\"German\",\n",
    "\"British\"]}}\n",
    "\n",
    "Now, complete the task.\n",
    "\n",
    "Input: {input} Output:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda5eaa5-33ea-4758-a043-921f1f1cbb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output schema using a Pydantic model\n",
    "class Output(BaseModel):\n",
    "    person: List[str]\n",
    "    organization: List[str]\n",
    "    location: List[str]\n",
    "    miscellaneous: List[str]\n",
    "\n",
    "# Schema and adapter ID vars for use later\n",
    "response_schema = Output.schema()\n",
    "adapter_id = \"<ADAPTER_ID>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fdc8e0-4766-477e-b787-fa9ebb67348f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dset = pd.read_csv(\"conllpp.csv\")\n",
    "test_set = dset[dset[\"split\"] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713b578a-3b56-4412-b651-e5a25a1de753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate LoRAX client\n",
    "client = Client(\"http://127.0.0.1:8080\")\n",
    "\n",
    "def generate(input, adapter_id=None, schema=None):\n",
    "    response_format = None if schema is None else {\"type\": \"json_object\", \"schema\": schema}\n",
    "\n",
    "    return client.generate(\n",
    "        prompt_template.format(input=input),\n",
    "        adapter_id=adapter_id,\n",
    "        adapter_source=\"s3\",\n",
    "        max_new_tokens=128,\n",
    "        response_format=response_format,\n",
    "        details=False,\n",
    "    )\n",
    "\n",
    "def benchmark(adapter_id=None, schema=None):\n",
    "    model_label = \"base\" if adapter_id is None else \"finetuned\"\n",
    "    schema_label = \"raw\" if schema is None else \"constrained\"\n",
    "    output_file = f\"benchmarks/{model_label}_{schema_label}.csv\"\n",
    "\n",
    "    print(f\"=====BENCHMARK:{model_label}+{schema_label}=====\\n\")\n",
    "\n",
    "    final_outputs = []\n",
    "    failed_rows = []\n",
    "    for idx, (_, row) in tqdm(enumerate(test_set.iterrows()), total=len(test_set), ncols=100):\n",
    "        try:\n",
    "            result = generate(row.input, adapter_id=adapter_id, schema=schema)\n",
    "            final_outputs.append(result.generated_text)\n",
    "        except:\n",
    "            print(f\"Failed generation for row {idx}\")\n",
    "            failed_rows.append(idx)\n",
    "\n",
    "    print(\"\\n========DONE========\")\n",
    "    print(f\"Success: {len(final_outputs)} Failed: {len(failed_rows)}\\n\")\n",
    "\n",
    "    print(\"Writing outputs to file...\")\n",
    "    final_outputs_df = pd.DataFrame(final_outputs)\n",
    "    final_outputs_df.to_csv(output_file, header=[\"output\"], index=False)\n",
    "\n",
    "\n",
    "# Run all four benchmarking combinations\n",
    "benchmark()\n",
    "benchmark(schema=response_schema)\n",
    "benchmark(adapter_id=adapter_id)\n",
    "benchmark(adapter_id=adapter_id, schema=response_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b2c352-59ff-4e47-b1db-27de37f8cc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYS = {'person', 'organization', 'location', 'miscellaneous'}\n",
    "ValidateJsonResult = namedtuple('Result', ['is_valid', 'reason'])\n",
    "\n",
    "def validate_json(json_str):\n",
    "    try:\n",
    "        d = json.loads(json_str)\n",
    "        d_keys = set(list(d.keys()))\n",
    "        if d_keys == KEYS:\n",
    "            return ValidateJsonResult(True, '')\n",
    "        else:\n",
    "            return ValidateJsonResult(False, f'missing json keys. expected: {KEYS}, got: :{d_keys}')\n",
    "    except Exception as e:\n",
    "        return ValidateJsonResult(False, f'invalid json (original_msg: {e}). got {json_str}')\n",
    "\n",
    "def flatten_dict(d):\n",
    "    res = []\n",
    "    for k, v in d.items():\n",
    "        assert isinstance(v, list)\n",
    "        res.append(f'__{k.upper()}__::{v}')\n",
    "    return res\n",
    "\n",
    "def compute_jaccard(target_dict, pred_dict):\n",
    "    target_entities = flatten_dict(target_dict)\n",
    "    pred_entities = flatten_dict(pred_dict)\n",
    "    target_count = Counter(target_entities)\n",
    "    pred_count = Counter(pred_entities)\n",
    "\n",
    "    num = 0\n",
    "    den = 0\n",
    "    all_keys = set(target_entities).union(set(pred_entities))\n",
    "    for key in all_keys:\n",
    "        num += min(target_count.get(key, 0), pred_count.get(key, 0))\n",
    "        den += max(target_count.get(key, 0), pred_count.get(key, 0))\n",
    "    return num / den\n",
    "\n",
    "def calc_results(benchmark_file):\n",
    "    is_valid_data = []\n",
    "    scores = []\n",
    "    preds_df = pd.read_csv(benchmark_file)\n",
    "    for idx, row in preds_df.iterrows():\n",
    "        validate_json_result = validate_json(row.output)\n",
    "        is_valid_data.append({\n",
    "            'is_valid': validate_json_result.is_valid,\n",
    "            'reason': validate_json_result.reason,\n",
    "        })\n",
    "\n",
    "        score = 0\n",
    "        if validate_json_result.is_valid:\n",
    "            target_dict = json.loads(test_set.iloc[idx].output)\n",
    "            pred_dict = json.loads(row.output)\n",
    "            score = compute_jaccard(target_dict, pred_dict)\n",
    "\n",
    "        scores.append(score)\n",
    "\n",
    "    return is_valid_data, scores\n",
    "\n",
    "# Calculate benchmarking results\n",
    "results_df = pd.DataFrame()\n",
    "reasons_df = pd.DataFrame()\n",
    "\n",
    "for f in sorted(os.listdir(\"benchmarks\")):\n",
    "    if not f.endswith(\".csv\"):\n",
    "        continue\n",
    "\n",
    "    is_valid_data, scores = calc_results(os.path.join(benchmarks_dir, f))\n",
    "    benchmark_name = f.split(\".\")[0]\n",
    "    results_df[f\"{benchmark_name}_score\"] = scores\n",
    "    reasons_df[f\"{benchmark_name}_valid\"] = [v[\"is_valid\"] for v in is_valid_data]\n",
    "    reasons_df[f\"{benchmark_name}_reason\"] = [v[\"reason\"] for v in is_valid_data]\n",
    "\n",
    "# Average Jaccard scores for each combination\n",
    "results_df.mean(axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
